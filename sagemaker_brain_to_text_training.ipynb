{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Brain-to-Text Training on AWS SageMaker\n",
        "\n",
        "This notebook trains the RNN baseline model for brain-to-text decoding using data from AWS S3.\n",
        "\n",
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(\"Installing required packages...\")\n",
        "\n",
        "# First, check if we need to install PyTorch with CUDA support\n",
        "import sys\n",
        "try:\n",
        "    import torch\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è  PyTorch CUDA not available. Installing PyTorch with CUDA support...\")\n",
        "        # Install PyTorch with CUDA support for CUDA 11.8 (common on SageMaker)\n",
        "        %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    else:\n",
        "        print(\"‚úÖ PyTorch with CUDA is already available\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch with CUDA support...\")\n",
        "    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install other required packages\n",
        "%pip install omegaconf h5py boto3 s3fs\n",
        "\n",
        "print(\"‚úÖ Package installation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio.functional as F\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import boto3\n",
        "import s3fs\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"üîç GPU Availability Check:\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
        "        print(f\"    Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
        "else:\n",
        "    print(\"‚ùå No CUDA GPUs available\")\n",
        "    print(\"\\nüîß Troubleshooting steps:\")\n",
        "    print(\"1. Check if NVIDIA drivers are installed:\")\n",
        "    print(\"   !nvidia-smi\")\n",
        "    print(\"2. Check if CUDA is properly installed:\")\n",
        "    print(\"   !nvcc --version\")\n",
        "    print(\"3. Verify PyTorch CUDA installation:\")\n",
        "    print(\"   !python -c 'import torch; print(torch.cuda.is_available())'\")\n",
        "\n",
        "# Additional system checks\n",
        "print(f\"\\nüñ•Ô∏è  System Information:\")\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"Platform: {os.sys.platform}\")\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'\\nUsing device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run GPU diagnostics\n",
        "print(\"üîç Running GPU Diagnostics...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check NVIDIA drivers\n",
        "print(\"1. NVIDIA Driver Check:\")\n",
        "try:\n",
        "    import subprocess\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ NVIDIA drivers are installed\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"‚ùå nvidia-smi command failed\")\n",
        "        print(\"Error:\", result.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running nvidia-smi: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Check CUDA installation\n",
        "print(\"2. CUDA Installation Check:\")\n",
        "try:\n",
        "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ CUDA is installed\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"‚ùå nvcc command failed\")\n",
        "        print(\"Error:\", result.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running nvcc: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Check PyTorch CUDA\n",
        "print(\"3. PyTorch CUDA Check:\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"‚ùå PyTorch cannot access CUDA\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Check environment variables\n",
        "print(\"4. Environment Variables:\")\n",
        "import os\n",
        "cuda_vars = ['CUDA_HOME', 'CUDA_PATH', 'LD_LIBRARY_PATH', 'PATH']\n",
        "for var in cuda_vars:\n",
        "    value = os.environ.get(var, 'Not set')\n",
        "    print(f\"  {var}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Diagnostics complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AWS S3 Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS S3 Configuration\n",
        "S3_BUCKET_NAME = '4k-woody-btt'\n",
        "S3_DATA_PREFIX = '4k/data/'\n",
        "LOCAL_DATA_DIR = '/tmp/brain_to_text_data'\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "s3_fs = s3fs.S3FileSystem()\n",
        "\n",
        "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"Data prefix: {S3_DATA_PREFIX}\")\n",
        "print(f\"Local data directory: {LOCAL_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test S3 path construction for sessions\n",
        "print(\"üîç Testing S3 Path Construction...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load the sessions from config\n",
        "from omegaconf import OmegaConf\n",
        "import os\n",
        "\n",
        "# Check current working directory and find the config file\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Files in current directory: {os.listdir('.')}\")\n",
        "\n",
        "# Try different possible locations for the config file\n",
        "config_paths = [\n",
        "    'rnn_args.yaml',\n",
        "    '../rnn_args.yaml', \n",
        "    '/home/ec2-user/SageMaker/btt_training/rnn_args.yaml',\n",
        "    '/home/ec2-user/SageMaker/btt_training/model_training/rnn_args.yaml'\n",
        "]\n",
        "\n",
        "config = None\n",
        "for config_path in config_paths:\n",
        "    if os.path.exists(config_path):\n",
        "        print(f\"‚úÖ Found config file at: {config_path}\")\n",
        "        config = OmegaConf.load(config_path)\n",
        "        break\n",
        "    else:\n",
        "        print(f\"‚ùå Config file not found at: {config_path}\")\n",
        "\n",
        "if config is None:\n",
        "    print(\"‚ùå Could not find rnn_args.yaml file!\")\n",
        "    print(\"Please check the file location and update the path.\")\n",
        "else:\n",
        "    sessions = config.dataset.sessions\n",
        "    print(f\"üìã Sessions from config: {sessions}\")\n",
        "    print(f\"üì¶ S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "    print(f\"üìÅ S3 Prefix: {S3_DATA_PREFIX}\")\n",
        "\n",
        "    print(\"\\nüîç Testing session paths:\")\n",
        "    for session in sessions[:3]:  # Test first 3 sessions\n",
        "        s3_path = f\"{S3_BUCKET_NAME}/{S3_DATA_PREFIX}{session}\"\n",
        "        print(f\"\\nSession: {session}\")\n",
        "        print(f\"  Constructed path: {s3_path}\")\n",
        "        \n",
        "        try:\n",
        "            # Test if the session directory exists\n",
        "            response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=f\"{S3_DATA_PREFIX}{session}/\", MaxKeys=5)\n",
        "            if 'Contents' in response:\n",
        "                print(f\"  ‚úÖ Session directory exists\")\n",
        "                print(f\"  üìÅ Files in session:\")\n",
        "                for obj in response['Contents']:\n",
        "                    print(f\"    - {obj['Key']}\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå Session directory not found\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error checking session: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Path construction test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the rnn_args.yaml file\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"üîç Searching for rnn_args.yaml file...\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Search for the config file in various locations\n",
        "search_paths = [\n",
        "    '.',\n",
        "    '..',\n",
        "    '/home/ec2-user/SageMaker/btt_training/',\n",
        "    '/home/ec2-user/SageMaker/btt_training/model_training/',\n",
        "    '/home/ec2-user/SageMaker/'\n",
        "]\n",
        "\n",
        "found_files = []\n",
        "for search_path in search_paths:\n",
        "    if os.path.exists(search_path):\n",
        "        pattern = os.path.join(search_path, '**', 'rnn_args.yaml')\n",
        "        files = glob.glob(pattern, recursive=True)\n",
        "        found_files.extend(files)\n",
        "\n",
        "if found_files:\n",
        "    print(f\"‚úÖ Found {len(found_files)} rnn_args.yaml file(s):\")\n",
        "    for file_path in found_files:\n",
        "        print(f\"  - {file_path}\")\n",
        "    \n",
        "    # Use the first found file\n",
        "    CONFIG_FILE_PATH = found_files[0]\n",
        "    print(f\"\\nüìÅ Using config file: {CONFIG_FILE_PATH}\")\n",
        "else:\n",
        "    print(\"‚ùå No rnn_args.yaml file found!\")\n",
        "    print(\"Please ensure the file exists in the repository.\")\n",
        "    CONFIG_FILE_PATH = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "t according "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S3 Direct Access Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_s3_data_files(bucket_name, prefix):\n",
        "    \"\"\"\n",
        "    List all HDF5 files in S3 bucket prefix\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # List all objects in the S3 prefix\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "        \n",
        "        h5_files = []\n",
        "        \n",
        "        for page in pages:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    s3_key = obj['Key']\n",
        "                    \n",
        "                    # Only include HDF5 files\n",
        "                    if s3_key.endswith('.hdf5'):\n",
        "                        h5_files.append(s3_key)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(h5_files)} HDF5 files in S3\")\n",
        "        return h5_files\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"‚ùå Error listing S3 files: {e}\")\n",
        "        return []\n",
        "\n",
        "# List available training data files\n",
        "print(\"Scanning S3 for training data files...\")\n",
        "s3_files = list_s3_data_files(S3_BUCKET_NAME, S3_DATA_PREFIX)\n",
        "\n",
        "# Show some example files\n",
        "if s3_files:\n",
        "    print(\"\\nExample S3 files:\")\n",
        "    for file_path in s3_files[:5]:  # Show first 5 files\n",
        "        print(f\"  - s3://{S3_BUCKET_NAME}/{file_path}\")\n",
        "    if len(s3_files) > 5:\n",
        "        print(f\"  ... and {len(s3_files) - 5} more files\")\n",
        "else:\n",
        "    print(\"‚ùå No HDF5 files found in S3. Please check your bucket and prefix configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone Repository and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Neuroprosthetics-Lab/nejm-brain-to-text.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change to the model_training directory\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the model_training directory to Python path\n",
        "sys.path.append('/home/ec2-user/SageMaker/btt_training/model_training')\n",
        "\n",
        "# Change to the model_training directory\n",
        "os.chdir('/home/ec2-user/SageMaker/btt_training/model_training')\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Python path updated for model_training module\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Training Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and modify configuration for SageMaker with S3 direct access\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "print(\"Loading original configuration...\")\n",
        "args = OmegaConf.load('rnn_args.yaml')\n",
        "\n",
        "# Update configuration for S3 direct access\n",
        "args.dataset.s3_bucket = S3_BUCKET_NAME\n",
        "args.dataset.s3_prefix = S3_DATA_PREFIX\n",
        "args.dataset.use_s3_direct = True  # Flag to use S3 direct access\n",
        "\n",
        "# Auto-configure GPU number based on availability\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    requested_gpu = int(args.gpu_number)\n",
        "    if requested_gpu >= num_gpus:\n",
        "        args.gpu_number = '0'  # Use GPU 0 if requested GPU doesn't exist\n",
        "        print(f\"‚ö†Ô∏è  Requested GPU {requested_gpu} not available. Using GPU 0 instead.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Using requested GPU {requested_gpu}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No CUDA GPUs available. Training will use CPU.\")\n",
        "\n",
        "# Update output directories for SageMaker\n",
        "args.output_dir = '/home/ec2-user/SageMaker/trained_models/baseline_rnn'\n",
        "args.checkpoint_dir = '/home/ec2-user/SageMaker/trained_models/baseline_rnn/checkpoint'\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nConfiguration updated for SageMaker with S3 direct access:\")\n",
        "print(f\"  S3 Bucket: {args.dataset.s3_bucket}\")\n",
        "print(f\"  S3 Prefix: {args.dataset.s3_prefix}\")\n",
        "print(f\"  Use S3 Direct: {args.dataset.use_s3_direct}\")\n",
        "print(f\"  GPU Number: {args.gpu_number}\")\n",
        "print(f\"  Output directory: {args.output_dir}\")\n",
        "print(f\"  Checkpoint directory: {args.checkpoint_dir}\")\n",
        "\n",
        "# S3 Checkpoint Configuration\n",
        "S3_CHECKPOINT_PREFIX = 'training_results/baseline_rnn/checkpoints/'\n",
        "S3_BEST_CHECKPOINT_KEY = f'{S3_CHECKPOINT_PREFIX}best_checkpoint'\n",
        "\n",
        "print(f\"  S3 Checkpoint prefix: s3://{S3_BUCKET_NAME}/{S3_CHECKPOINT_PREFIX}\")\n",
        "print(f\"  S3 Best checkpoint: s3://{S3_BUCKET_NAME}/{S3_BEST_CHECKPOINT_KEY}\")\n",
        "\n",
        "# Optional: Resume from S3 checkpoint if available\n",
        "# RESUME_FROM_S3 = True\n",
        "# S3_CHECKPOINT_TO_RESUME = f'{S3_CHECKPOINT_PREFIX}checkpoint_step_10000'  # Example checkpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and run the training with S3 direct access\n",
        "from s3_rnn_trainer import S3BrainToTextDecoder_Trainer\n",
        "\n",
        "print(\"Starting training with S3 direct access...\")\n",
        "print(f\"S3 Bucket: {args.dataset.s3_bucket}\")\n",
        "print(f\"S3 Prefix: {args.dataset.s3_prefix}\")\n",
        "print(f\"Output directory: {args.output_dir}\")\n",
        "print(f\"Number of training batches: {args.num_training_batches}\")\n",
        "print(f\"Batch size: {args.dataset.batch_size}\")\n",
        "print(f\"Learning rate: {args.lr_max}\")\n",
        "\n",
        "# Create the S3 trainer\n",
        "trainer = S3BrainToTextDecoder_Trainer(args)\n",
        "\n",
        "# Optional: Resume from S3 checkpoint\n",
        "# Uncomment the following lines to resume from a specific S3 checkpoint:\n",
        "# if 'RESUME_FROM_S3' in locals() and RESUME_FROM_S3:\n",
        "#     print(f\"Resuming from S3 checkpoint: {S3_CHECKPOINT_TO_RESUME}\")\n",
        "#     start_step = trainer.resume_from_s3_checkpoint(S3_CHECKPOINT_TO_RESUME)\n",
        "#     print(f\"Resumed from step {start_step}\")\n",
        "\n",
        "# Run training\n",
        "metrics = trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "print(f\"Final metrics: {metrics}\")\n",
        "\n",
        "# Clean up cached files\n",
        "trainer.cleanup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S3 Checkpoint Management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_s3_checkpoints(bucket_name, prefix):\n",
        "    \"\"\"List all checkpoints in S3\"\"\"\n",
        "    try:\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "        \n",
        "        checkpoints = []\n",
        "        for page in pages:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    key = obj['Key']\n",
        "                    if key.endswith('.pt') or 'checkpoint' in key:\n",
        "                        checkpoints.append({\n",
        "                            'key': key,\n",
        "                            'size': obj['Size'],\n",
        "                            'last_modified': obj['LastModified']\n",
        "                        })\n",
        "        \n",
        "        return sorted(checkpoints, key=lambda x: x['last_modified'], reverse=True)\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"Error listing S3 checkpoints: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_checkpoint_from_s3(bucket_name, s3_key, local_path):\n",
        "    \"\"\"Download a specific checkpoint from S3\"\"\"\n",
        "    try:\n",
        "        s3_client.download_file(bucket_name, s3_key, local_path)\n",
        "        print(f\"Downloaded checkpoint to: {local_path}\")\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        print(f\"Error downloading checkpoint: {e}\")\n",
        "        return False\n",
        "\n",
        "# List available checkpoints in S3\n",
        "print(\"Available checkpoints in S3:\")\n",
        "checkpoints = list_s3_checkpoints(S3_BUCKET_NAME, S3_CHECKPOINT_PREFIX)\n",
        "\n",
        "if checkpoints:\n",
        "    for i, checkpoint in enumerate(checkpoints[:10]):  # Show first 10\n",
        "        print(f\"  {i+1}. {checkpoint['key']}\")\n",
        "        print(f\"     Size: {checkpoint['size']} bytes\")\n",
        "        print(f\"     Modified: {checkpoint['last_modified']}\")\n",
        "        print()\n",
        "    if len(checkpoints) > 10:\n",
        "        print(f\"  ... and {len(checkpoints) - 10} more checkpoints\")\n",
        "else:\n",
        "    print(\"  No checkpoints found in S3 yet.\")\n",
        "    print(\"  Checkpoints will be saved during training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Results to S3 (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_s3(local_dir, bucket_name, s3_prefix):\n",
        "    \"\"\"\n",
        "    Upload local directory contents to S3\n",
        "    \"\"\"\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(local_dir):\n",
        "            for file in files:\n",
        "                local_file_path = os.path.join(root, file)\n",
        "                relative_path = os.path.relpath(local_file_path, local_dir)\n",
        "                s3_key = f\"{s3_prefix}{relative_path}\"\n",
        "                \n",
        "                print(f\"Uploading: {local_file_path} -> s3://{bucket_name}/{s3_key}\")\n",
        "                s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
        "        \n",
        "        print(\"\\n‚úÖ Successfully uploaded results to S3\")\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"‚ùå Error uploading to S3: {e}\")\n",
        "\n",
        "# Upload trained models and results to S3\n",
        "print(\"Uploading training results to S3...\")\n",
        "upload_to_s3(args.output_dir, S3_BUCKET_NAME, 'training_results/baseline_rnn/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset: S3 bucket '{S3_BUCKET_NAME}' at '{S3_DATA_PREFIX}'\")\n",
        "print(f\"Data access: Direct S3 access (no local download)\")\n",
        "print(f\"Output directory: {args.output_dir}\")\n",
        "print(f\"Training batches: {args.num_training_batches}\")\n",
        "print(f\"Batch size: {args.dataset.batch_size}\")\n",
        "print(f\"Model architecture: {args.model.n_layers} GRU layers with {args.model.n_units} units each\")\n",
        "print(f\"Final metrics: {metrics}\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ Training completed successfully with S3 direct access!\")\n",
        "print(\"üí° Benefits: No need to download entire dataset, faster startup, less storage usage\")\n",
        "print(\"=\"*50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
