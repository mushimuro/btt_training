{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Brain-to-Text Training on AWS SageMaker\n",
        "\n",
        "This notebook trains the RNN baseline model for brain-to-text decoding using data from AWS S3.\n",
        "\n",
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install omegaconf h5py torchaudio boto3 s3fs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio.functional as F\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import boto3\n",
        "import s3fs\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AWS S3 Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS S3 Configuration\n",
        "S3_BUCKET_NAME = '4k-woody-btt'\n",
        "S3_DATA_PREFIX = '4k/data/'\n",
        "LOCAL_DATA_DIR = '/tmp/brain_to_text_data'\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "s3_fs = s3fs.S3FileSystem()\n",
        "\n",
        "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"Data prefix: {S3_DATA_PREFIX}\")\n",
        "print(f\"Local data directory: {LOCAL_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S3 Direct Access Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_s3_data_files(bucket_name, prefix):\n",
        "    \"\"\"\n",
        "    List all HDF5 files in S3 bucket prefix\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # List all objects in the S3 prefix\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "        \n",
        "        h5_files = []\n",
        "        \n",
        "        for page in pages:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    s3_key = obj['Key']\n",
        "                    \n",
        "                    # Only include HDF5 files\n",
        "                    if s3_key.endswith('.hdf5'):\n",
        "                        h5_files.append(s3_key)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(h5_files)} HDF5 files in S3\")\n",
        "        return h5_files\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"‚ùå Error listing S3 files: {e}\")\n",
        "        return []\n",
        "\n",
        "# List available training data files\n",
        "print(\"Scanning S3 for training data files...\")\n",
        "s3_files = list_s3_data_files(S3_BUCKET_NAME, S3_DATA_PREFIX)\n",
        "\n",
        "# Show some example files\n",
        "if s3_files:\n",
        "    print(\"\\nExample S3 files:\")\n",
        "    for file_path in s3_files[:5]:  # Show first 5 files\n",
        "        print(f\"  - s3://{S3_BUCKET_NAME}/{file_path}\")\n",
        "    if len(s3_files) > 5:\n",
        "        print(f\"  ... and {len(s3_files) - 5} more files\")\n",
        "else:\n",
        "    print(\"‚ùå No HDF5 files found in S3. Please check your bucket and prefix configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone Repository and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Neuroprosthetics-Lab/nejm-brain-to-text.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change to the model_training directory\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the model_training directory to Python path\n",
        "sys.path.append('/home/ec2-user/SageMaker/nejm-brain-to-text/model_training')\n",
        "\n",
        "# Change to the model_training directory\n",
        "os.chdir('/home/ec2-user/SageMaker/nejm-brain-to-text/model_training')\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Python path updated for model_training module\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Training Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and modify configuration for SageMaker with S3 direct access\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "print(\"Loading original configuration...\")\n",
        "args = OmegaConf.load('rnn_args.yaml')\n",
        "\n",
        "# Update configuration for S3 direct access\n",
        "args.dataset.s3_bucket = S3_BUCKET_NAME\n",
        "args.dataset.s3_prefix = S3_DATA_PREFIX\n",
        "args.dataset.use_s3_direct = True  # Flag to use S3 direct access\n",
        "\n",
        "# Update output directories for SageMaker\n",
        "args.output_dir = '/home/ec2-user/SageMaker/trained_models/baseline_rnn'\n",
        "args.checkpoint_dir = '/home/ec2-user/SageMaker/trained_models/baseline_rnn/checkpoint'\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(\"Configuration updated for SageMaker with S3 direct access:\")\n",
        "print(f\"  S3 Bucket: {args.dataset.s3_bucket}\")\n",
        "print(f\"  S3 Prefix: {args.dataset.s3_prefix}\")\n",
        "print(f\"  Use S3 Direct: {args.dataset.use_s3_direct}\")\n",
        "print(f\"  Output directory: {args.output_dir}\")\n",
        "print(f\"  Checkpoint directory: {args.checkpoint_dir}\")\n",
        "\n",
        "# Optional: Resume from checkpoint if available\n",
        "# args.init_from_checkpoint = True\n",
        "# args.init_checkpoint_path = '/home/ec2-user/SageMaker/trained_models/baseline_rnn/checkpoint/best_checkpoint'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and run the training with S3 direct access\n",
        "from s3_rnn_trainer import S3BrainToTextDecoder_Trainer\n",
        "\n",
        "print(\"Starting training with S3 direct access...\")\n",
        "print(f\"S3 Bucket: {args.dataset.s3_bucket}\")\n",
        "print(f\"S3 Prefix: {args.dataset.s3_prefix}\")\n",
        "print(f\"Output directory: {args.output_dir}\")\n",
        "print(f\"Number of training batches: {args.num_training_batches}\")\n",
        "print(f\"Batch size: {args.dataset.batch_size}\")\n",
        "print(f\"Learning rate: {args.lr_max}\")\n",
        "\n",
        "# Create the S3 trainer and run training\n",
        "trainer = S3BrainToTextDecoder_Trainer(args)\n",
        "metrics = trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "print(f\"Final metrics: {metrics}\")\n",
        "\n",
        "# Clean up cached files\n",
        "trainer.cleanup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Results to S3 (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_s3(local_dir, bucket_name, s3_prefix):\n",
        "    \"\"\"\n",
        "    Upload local directory contents to S3\n",
        "    \"\"\"\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(local_dir):\n",
        "            for file in files:\n",
        "                local_file_path = os.path.join(root, file)\n",
        "                relative_path = os.path.relpath(local_file_path, local_dir)\n",
        "                s3_key = f\"{s3_prefix}{relative_path}\"\n",
        "                \n",
        "                print(f\"Uploading: {local_file_path} -> s3://{bucket_name}/{s3_key}\")\n",
        "                s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
        "        \n",
        "        print(\"\\n‚úÖ Successfully uploaded results to S3\")\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"‚ùå Error uploading to S3: {e}\")\n",
        "\n",
        "# Upload trained models and results to S3\n",
        "print(\"Uploading training results to S3...\")\n",
        "upload_to_s3(args.output_dir, S3_BUCKET_NAME, 'training_results/baseline_rnn/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset: S3 bucket '{S3_BUCKET_NAME}' at '{S3_DATA_PREFIX}'\")\n",
        "print(f\"Data access: Direct S3 access (no local download)\")\n",
        "print(f\"Output directory: {args.output_dir}\")\n",
        "print(f\"Training batches: {args.num_training_batches}\")\n",
        "print(f\"Batch size: {args.dataset.batch_size}\")\n",
        "print(f\"Model architecture: {args.model.n_layers} GRU layers with {args.model.n_units} units each\")\n",
        "print(f\"Final metrics: {metrics}\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ Training completed successfully with S3 direct access!\")\n",
        "print(\"üí° Benefits: No need to download entire dataset, faster startup, less storage usage\")\n",
        "print(\"=\"*50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
